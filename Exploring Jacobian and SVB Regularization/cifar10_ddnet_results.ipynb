{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of DDNet Trained on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I display results in terms of accuracy on test data for DDNet models trained on the CIFAR10 dataset using various regularization techniques. The DDNet model and the CIFAR10 dataset were set up as in Hoffman 2019 (for all the models). The CIFAR10 data is preprocessed by normalizing using mean [0.5, 0.5, 0.5] and variance [0.5, 0.5, 0.5]. The batch size is 100. The model optimizes using SGD with momentum p = 0.9, and standard cross-entropy loss. Model parameters are initialized using Glorot initialization (See Glorot & Bengio 2010), expect for SVB regularization which uses orthogonal initialization. Models are trained with no regularization, L2 regularization, SVB regularization and Jacobian regularization with a dropout rate of p_drop = 0.5. I also train a model with Jacobian Regularization without Dropout. The L2 regularization coefficient and Jacobian regularization coefficient are the same as in Hoffman 2019: l2_lmbd = 0.0005 and lambda_jacobian_reg = 0.01. For SVB regularization I use the hyperparameters svb_freq=600 and svb_eps = 0.05. The learning rate starts at 0.1, and is reduced to 0.01 and 0.001 1/3 and 2/3s into training, respectively. The models are trained for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy import stats\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_generators import data_loader_CIFAR10\n",
    "from model_classes import DDNet\n",
    "from plotting_tools import get_random_img, generate_random_vectors\n",
    "from tools import accuracy, compute_total_variation, ModelInfo\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "            Conv2d-2           [-1, 64, 28, 28]          36,928\n",
      "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
      "            Conv2d-4          [-1, 128, 12, 12]          73,856\n",
      "            Conv2d-5          [-1, 128, 10, 10]         147,584\n",
      "         MaxPool2d-6            [-1, 128, 5, 5]               0\n",
      "            Linear-7                  [-1, 256]         819,456\n",
      "           Dropout-8                  [-1, 256]               0\n",
      "            Linear-9                  [-1, 256]          65,792\n",
      "          Dropout-10                  [-1, 256]               0\n",
      "           Linear-11                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 1,147,978\n",
      "Trainable params: 1,147,978\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.19\n",
      "Params size (MB): 4.38\n",
      "Estimated Total Size (MB): 5.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST data\n",
    "train_loader, test_loader = data_loader_CIFAR10()\n",
    "\n",
    "# Summary of model\n",
    "summary_model = DDNet().to(device)\n",
    "summary(summary_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names_set:\n\u001b[0;32m     15\u001b[0m         model_names\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m models \u001b[39m=\u001b[39m {name: ModelInfo(name, dataset) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names}\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names_set:\n\u001b[0;32m     15\u001b[0m         model_names\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m models \u001b[39m=\u001b[39m {name: ModelInfo(name, dataset) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names}\n",
      "File \u001b[1;32mc:\\Users\\aslak\\Desktop\\Jobb\\Forskningsassistent\\Biologisk inspirert kunstig intelligens\\regNNs\\Exploring Jacobian and SVB Regularization\\tools.py:286\u001b[0m, in \u001b[0;36mModelInfo.__init__\u001b[1;34m(self, name, dataset)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n\u001b[0;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m dataset\n\u001b[0;32m    279\u001b[0m (\n\u001b[0;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses,\n\u001b[0;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_losses,\n\u001b[0;32m    283\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs,\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_accuracies,\n\u001b[0;32m    285\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_accuracies,\n\u001b[1;32m--> 286\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_model(name, dataset)\n",
      "File \u001b[1;32mc:\\Users\\aslak\\Desktop\\Jobb\\Forskningsassistent\\Biologisk inspirert kunstig intelligens\\regNNs\\Exploring Jacobian and SVB Regularization\\tools.py:311\u001b[0m, in \u001b[0;36mModelInfo.load_model\u001b[1;34m(name, dataset)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(name, dataset):\n\u001b[0;32m    290\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39m    This static method is used to load the model and related information from a file using load_trained_model.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39m    test_accuracies (list): The loaded list of test accuracies.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m     (\n\u001b[0;32m    305\u001b[0m         model,\n\u001b[0;32m    306\u001b[0m         losses,\n\u001b[0;32m    307\u001b[0m         reg_losses,\n\u001b[0;32m    308\u001b[0m         epochs,\n\u001b[0;32m    309\u001b[0m         train_accuracies,\n\u001b[0;32m    310\u001b[0m         test_accuracies,\n\u001b[1;32m--> 311\u001b[0m     ) \u001b[39m=\u001b[39m load_trained_model(name, dataset)\n\u001b[0;32m    312\u001b[0m     \u001b[39mreturn\u001b[39;00m model, losses, reg_losses, epochs, train_accuracies, test_accuracies\n",
      "File \u001b[1;32mc:\\Users\\aslak\\Desktop\\Jobb\\Forskningsassistent\\Biologisk inspirert kunstig intelligens\\regNNs\\Exploring Jacobian and SVB Regularization\\tools.py:244\u001b[0m, in \u001b[0;36mload_trained_model\u001b[1;34m(model_name, dataset)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39m# Load training data\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./trained_\u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m}\u001b[39;00m\u001b[39m_models/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_data.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 244\u001b[0m     data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m    246\u001b[0m losses \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mlosses\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    247\u001b[0m reg_losses \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mreg_losses\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\storage.py:241\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(io\u001b[39m.\u001b[39mBytesIO(b))\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1041\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1042\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1043\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39mload()\n\u001b[0;32m   1045\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1047\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    976\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    978\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m--> 980\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[0;32m    981\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    982\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[0;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "dataset = \"cifar10\"\n",
    "\n",
    "model_names_set = [\n",
    "    \"model_no_reg\",\n",
    "    \"model_l2\",\n",
    "    \"model_jacobi\",\n",
    "    \"model_jacobi_no_dropout\",\n",
    "    \"model_svb\",\n",
    "]\n",
    "model_names = []\n",
    "\n",
    "for i in range(5):\n",
    "    for name in model_names_set:\n",
    "        model_names.append(f\"{name}_{i}\")\n",
    "\n",
    "models = {name: ModelInfo(name, dataset) for name in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I calculate the accuracies of each model as a 95% confidence interval. I have trained five models of each variation that I use to calculate the CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold accuracy data\n",
    "accuracy_data = {}\n",
    "\n",
    "# Calculate accuracies with 95% CI for models\n",
    "for name in model_names_set:\n",
    "    accuracies = []\n",
    "    for i in range(5):  # Assuming 5 versions of each model\n",
    "        model_name = f\"{name}_{i}\"\n",
    "        model = models[model_name].model\n",
    "        acc = accuracy(model, test_loader, device)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "    # Calculate standard error\n",
    "    std_error = stats.sem(accuracies)\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    CI = std_error * stats.t.ppf((1 + 0.95) / 2, len(accuracies) - 1)\n",
    "\n",
    "    # Store results in the data dictionary\n",
    "    accuracy_data[f\"{name}\"] = (mean_accuracy, CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(\n",
    "    accuracy_data, orient=\"index\", columns=[\"Mean Accuracy\", \"Confidence Interval\"]\n",
    ")\n",
    "\n",
    "# Convert the numbers from decimal to percentage and round to four decimal places\n",
    "df = df.multiply(100).round(3)\n",
    "\n",
    "# Create a new column that combines Mean Accuracy and Confidence Interval\n",
    "df[\"Accuracy +/- CI (95%)\"] = df.apply(\n",
    "    lambda row: f\"{row['Mean Accuracy']} +/- {row['Confidence Interval']}\", axis=1\n",
    ")\n",
    "\n",
    "# Drop the original columns\n",
    "df = df.drop(columns=[\"Mean Accuracy\", \"Confidence Interval\"])\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total variation is a measure for roughness/complexity in images. I generate 25 different images for each model to get a good mean and standard deviation, and give the results as a tabel for each model with the eight models (models with and without dropout) as rows, and the three different zoom levels as columns. The tabel contains the mean and 95 % confidence interval for the total variation for each model at each zoom level, for both isotropic and anisotropic total variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isotropic Total Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained results\n",
    "\n",
    "# Define zoom levels\n",
    "zoom_levels = [0.025, 0.01, 0.001]\n",
    "\n",
    "# Define the models\n",
    "model_names = [\n",
    "    \"model_no_reg_0\",\n",
    "    \"model_l2_0\",\n",
    "    \"model_jacobi_0\",\n",
    "    \"model_jacobi_no_dropout_0\",\n",
    "    \"model_svb_0\",\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "cols = pd.MultiIndex.from_product([zoom_levels, [\"mean\", \"conf_interval\"]])\n",
    "df_results_isotropic = pd.DataFrame(index=model_names, columns=cols)\n",
    "\n",
    "# Loop over the selected models\n",
    "for name in tqdm(model_names):\n",
    "    # Load the results\n",
    "    with open(\n",
    "        f\"./total_variation_{dataset}_models/{name}_total_isotropic_variation.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        df_results = pickle.load(f)\n",
    "\n",
    "    # Copy the results to the dataframe\n",
    "    for zoom in zoom_levels:\n",
    "        df_results_isotropic.loc[name, (zoom, \"mean\")] = df_results.loc[\n",
    "            name, (zoom, \"mean\")\n",
    "        ]\n",
    "        df_results_isotropic.loc[name, (zoom, \"conf_interval\")] = df_results.loc[\n",
    "            name, (zoom, \"conf_interval\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Isotropic Total variation for three different zoom levels (mean and 95% CI for 50 generated images at each zoom level)\"\n",
    ")\n",
    "display(df_results_isotropic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anisotropic Total Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained results\n",
    "\n",
    "# Define zoom levels\n",
    "zoom_levels = [0.025, 0.01, 0.001]\n",
    "\n",
    "# Define the models\n",
    "model_names = [\n",
    "    \"model_no_reg_0\",\n",
    "    \"model_l2_0\",\n",
    "    \"model_jacobi_0\",\n",
    "    \"model_jacobi_no_dropout_0\",\n",
    "    \"model_svb_0\",\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "cols = pd.MultiIndex.from_product([zoom_levels, [\"mean\", \"conf_interval\"]])\n",
    "df_results_anisotropic = pd.DataFrame(index=model_names, columns=cols)\n",
    "\n",
    "# Loop over the selected models\n",
    "for name in tqdm(model_names):\n",
    "    # Load the results\n",
    "    with open(\n",
    "        f\"./total_variation_{dataset}_models/{name}_total_anisotropic_variation.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        df_results = pickle.load(f)\n",
    "\n",
    "    # Copy the results to the dataframe\n",
    "    for zoom in zoom_levels:\n",
    "        df_results_anisotropic.loc[name, (zoom, \"mean\")] = df_results.loc[\n",
    "            name, (zoom, \"mean\")\n",
    "        ]\n",
    "        df_results_anisotropic.loc[name, (zoom, \"conf_interval\")] = df_results.loc[\n",
    "            name, (zoom, \"conf_interval\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Anisotropic Total variation for three different zoom levels (mean and 95% CI for 50 generated images at each zoom level)\"\n",
    ")\n",
    "display(df_results_anisotropic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
