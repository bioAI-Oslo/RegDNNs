{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of ResNet18 Trained on CIFAR100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I display results in terms of accuracy on test data for torchvisions' ResNet18 models trained on the CIFAR100 dataset using various regularization techniques. The CIFAR100 data is preprocessed by normalizing using mean [0.5071, 0.4865, 0.4409] and variance [0.2009, 0.1984, 0.2023]. The batch size is 100. The model optimizes using SGD with momentum p = 0.9, and standard cross-entropy loss. Models are trained with no regularization, L2 regularization, SVB regularization and Jacobian regularization with a dropout rate of p_drop = 0.5. I also train a model with Jacobian Regularization without Dropout. The L2 regularization coefficient and Jacobian regularization coefficient are the same as in Hoffman 2019: l2_lmbd = 0.0005 and lambda_jacobian_reg = 0.01. For SVB regularization I use the hyperparameters svb_freq=600 and svb_eps = 0.05. The learning rate starts at 0.1, and is reduced to 0.01 and 0.001 1/3 and 2/3s into training, respectively. The models are trained for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from scipy import stats\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_generators import data_loader_CIFAR100\n",
    "from model_classes import ResNet18\n",
    "from tools import accuracy, ModelInfo\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 100]          51,300\n",
      "           ResNet-69                  [-1, 100]               0\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 44.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST data\n",
    "train_loader, test_loader = data_loader_CIFAR100()\n",
    "\n",
    "# Summary of model\n",
    "summary_model = ResNet18().to(device)\n",
    "summary(summary_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './trained_cifar100_models/model_no_reg_1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names_set:\n\u001b[0;32m     15\u001b[0m         model_names\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m models \u001b[39m=\u001b[39m {name: ModelInfo(name, dataset) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names}\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names_set:\n\u001b[0;32m     15\u001b[0m         model_names\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m models \u001b[39m=\u001b[39m {name: ModelInfo(name, dataset) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names}\n",
      "File \u001b[1;32mc:\\Users\\aslak\\Desktop\\Jobb\\Forskningsassistent\\Biologisk inspirert kunstig intelligens\\regNNs\\Exploring Jacobian and SVB Regularization\\tools.py:286\u001b[0m, in \u001b[0;36mModelInfo.__init__\u001b[1;34m(self, name, dataset)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n\u001b[0;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m dataset\n\u001b[0;32m    279\u001b[0m (\n\u001b[0;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses,\n\u001b[0;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_losses,\n\u001b[0;32m    283\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs,\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_accuracies,\n\u001b[0;32m    285\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_accuracies,\n\u001b[1;32m--> 286\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_model(name, dataset)\n",
      "File \u001b[1;32mc:\\Users\\aslak\\Desktop\\Jobb\\Forskningsassistent\\Biologisk inspirert kunstig intelligens\\regNNs\\Exploring Jacobian and SVB Regularization\\tools.py:311\u001b[0m, in \u001b[0;36mModelInfo.load_model\u001b[1;34m(name, dataset)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(name, dataset):\n\u001b[0;32m    290\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39m    This static method is used to load the model and related information from a file using load_trained_model.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39m    test_accuracies (list): The loaded list of test accuracies.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m     (\n\u001b[0;32m    305\u001b[0m         model,\n\u001b[0;32m    306\u001b[0m         losses,\n\u001b[0;32m    307\u001b[0m         reg_losses,\n\u001b[0;32m    308\u001b[0m         epochs,\n\u001b[0;32m    309\u001b[0m         train_accuracies,\n\u001b[0;32m    310\u001b[0m         test_accuracies,\n\u001b[1;32m--> 311\u001b[0m     ) \u001b[39m=\u001b[39m load_trained_model(name, dataset)\n\u001b[0;32m    312\u001b[0m     \u001b[39mreturn\u001b[39;00m model, losses, reg_losses, epochs, train_accuracies, test_accuracies\n",
      "File \u001b[1;32mc:\\Users\\aslak\\Desktop\\Jobb\\Forskningsassistent\\Biologisk inspirert kunstig intelligens\\regNNs\\Exploring Jacobian and SVB Regularization\\tools.py:226\u001b[0m, in \u001b[0;36mload_trained_model\u001b[1;34m(model_name, dataset)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError: Dataset not implemented\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[39m# Load state dictionary\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\n\u001b[0;32m    227\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./trained_\u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m}\u001b[39;00m\u001b[39m_models/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m, map_location\u001b[39m=\u001b[39mdevice\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    230\u001b[0m \u001b[39m# Create new OrderedDict that does not contain `module.` prefix\u001b[39;00m\n\u001b[0;32m    231\u001b[0m new_state_dict \u001b[39m=\u001b[39m OrderedDict()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './trained_cifar100_models/model_no_reg_1.pt'"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "dataset = \"cifar100\"\n",
    "\n",
    "model_names_set = [\n",
    "    \"model_no_reg\",\n",
    "    \"model_l2\",\n",
    "    \"model_jacobi\",\n",
    "    \"model_jacobi_no_dropout\",\n",
    "    \"model_svb\",\n",
    "]\n",
    "model_names = []\n",
    "\n",
    "for i in range(5):\n",
    "    for name in model_names_set:\n",
    "        model_names.append(f\"{name}_{i}\")\n",
    "\n",
    "models = {name: ModelInfo(name, dataset) for name in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I calculate the accuracies of each model as a 95% confidence interval. I have trained five models of each variation that I use to calculate the CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold accuracy data\n",
    "accuracy_data = {}\n",
    "\n",
    "# Calculate accuracies with 95% CI for models\n",
    "for name in model_names_set:\n",
    "    accuracies = []\n",
    "    for i in range(5):  # Assuming 5 versions of each model\n",
    "        model_name = f\"{name}_{i}\"\n",
    "        model = models[model_name].model\n",
    "        acc = accuracy(model, test_loader, device)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "    # Calculate standard error\n",
    "    std_error = stats.sem(accuracies)\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    CI = std_error * stats.t.ppf((1 + 0.95) / 2, len(accuracies) - 1)\n",
    "\n",
    "    # Store results in the data dictionary\n",
    "    accuracy_data[f\"{name}\"] = (mean_accuracy, CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(\n",
    "    accuracy_data, orient=\"index\", columns=[\"Mean Accuracy\", \"Confidence Interval\"]\n",
    ")\n",
    "\n",
    "# Convert the numbers from decimal to percentage and round to four decimal places\n",
    "df = df.multiply(100).round(3)\n",
    "\n",
    "# Create a new column that combines Mean Accuracy and Confidence Interval\n",
    "df[\"Accuracy +/- CI (95%)\"] = df.apply(\n",
    "    lambda row: f\"{row['Mean Accuracy']} +/- {row['Confidence Interval']}\", axis=1\n",
    ")\n",
    "\n",
    "# Drop the original columns\n",
    "df = df.drop(columns=[\"Mean Accuracy\", \"Confidence Interval\"])\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total variation is a measure for roughness/complexity in images. I generate 25 different images for each model to get a good mean and standard deviation, and give the results as a tabel for each model with the eight models (models with and without dropout) as rows, and the three different zoom levels as columns. The tabel contains the mean and 95 % confidence interval for the total variation for each model at each zoom level, for both isotropic and anisotropic total variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isotropic Total Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained results\n",
    "\n",
    "# Define zoom levels\n",
    "zoom_levels = [0.025, 0.01, 0.001]\n",
    "\n",
    "# Define the models\n",
    "model_names = [\n",
    "    \"model_no_reg_0\",\n",
    "    \"model_l2_0\",\n",
    "    \"model_jacobi_0\",\n",
    "    \"model_jacobi_no_dropout_0\",\n",
    "    \"model_svb_0\",\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "cols = pd.MultiIndex.from_product([zoom_levels, [\"mean\", \"conf_interval\"]])\n",
    "df_results_isotropic = pd.DataFrame(index=model_names, columns=cols)\n",
    "\n",
    "# Loop over the selected models\n",
    "for name in tqdm(model_names):\n",
    "    # Load the results\n",
    "    with open(\n",
    "        f\"./total_variation_{dataset}_models/{name}_total_isotropic_variation.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        df_results = pickle.load(f)\n",
    "\n",
    "    # Copy the results to the dataframe\n",
    "    for zoom in zoom_levels:\n",
    "        df_results_isotropic.loc[name, (zoom, \"mean\")] = df_results.loc[\n",
    "            name, (zoom, \"mean\")\n",
    "        ]\n",
    "        df_results_isotropic.loc[name, (zoom, \"conf_interval\")] = df_results.loc[\n",
    "            name, (zoom, \"conf_interval\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Isotropic Total variation for three different zoom levels (mean and 95% CI for 50 generated images at each zoom level)\"\n",
    ")\n",
    "display(df_results_isotropic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anisotropic Total Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained results\n",
    "\n",
    "# Define zoom levels\n",
    "zoom_levels = [0.025, 0.01, 0.001]\n",
    "\n",
    "# Define the models\n",
    "model_names = [\n",
    "    \"model_no_reg_0\",\n",
    "    \"model_l2_0\",\n",
    "    \"model_jacobi_0\",\n",
    "    \"model_jacobi_no_dropout_0\",\n",
    "    \"model_svb_0\",\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "cols = pd.MultiIndex.from_product([zoom_levels, [\"mean\", \"conf_interval\"]])\n",
    "df_results_anisotropic = pd.DataFrame(index=model_names, columns=cols)\n",
    "\n",
    "# Loop over the selected models\n",
    "for name in tqdm(model_names):\n",
    "    # Load the results\n",
    "    with open(\n",
    "        f\"./total_variation_{dataset}_models/{name}_total_anisotropic_variation.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        df_results = pickle.load(f)\n",
    "\n",
    "    # Copy the results to the dataframe\n",
    "    for zoom in zoom_levels:\n",
    "        df_results_anisotropic.loc[name, (zoom, \"mean\")] = df_results.loc[\n",
    "            name, (zoom, \"mean\")\n",
    "        ]\n",
    "        df_results_anisotropic.loc[name, (zoom, \"conf_interval\")] = df_results.loc[\n",
    "            name, (zoom, \"conf_interval\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Anisotropic Total variation for three different zoom levels (mean and 95% CI for 50 generated images at each zoom level)\"\n",
    ")\n",
    "display(df_results_anisotropic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
