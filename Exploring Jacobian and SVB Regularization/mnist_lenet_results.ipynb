{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of LeNet Trained on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I display results in terms of accuracy on test data for LeNet models trained on the MNIST dataset using various regularization techniques. The LeNet model and the MNIST dataset were set up as in Hoffman 2019 (for all the models). The MNIST data is preprocessed by normalizing using mean 0.1307 and variance 0.3081. The batch size is 100. The model optimizes using SGD with momentum p = 0.9, and standard cross-entropy loss. Model parameters are initialized using Glorot initialization (See Glorot & Bengio 2010), expect for SVB regularization which uses orthogonal initialization. Models are trained with no regularization, L2 regularization, SVB regularization and Jacobian regularization, both with and without dropout with a dropout rate of p_drop = 0.5. The L2 regularization coefficient and Jacobian regularization coefficient are the same as in Hoffman 2019: l2_lmbd = 0.0005 and lambda_jacobian_reg = 0.01. For SVB regularization I use the hyperparameters svb_freq=600 and svb_eps = 0.05. The learning rate starts at 0.1, and is reduced to 0.01 and 0.001 1/3 and 2/3s into training, respectively. The models are trained for 250 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy import stats\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_generators import data_loader_MNIST\n",
    "from model_classes import LeNet_MNIST\n",
    "from plotting_tools import get_random_img, generate_random_vectors\n",
    "from tools import accuracy, compute_total_variation, ModelInfo\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Linear-5                  [-1, 120]          48,120\n",
      "           Dropout-6                  [-1, 120]               0\n",
      "            Linear-7                   [-1, 84]          10,164\n",
      "           Dropout-8                   [-1, 84]               0\n",
      "            Linear-9                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST data\n",
    "train_loader, test_loader = data_loader_MNIST()\n",
    "\n",
    "# Summary of model\n",
    "summary_model = LeNet_MNIST().to(device)\n",
    "summary(summary_model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "\n",
    "# Defining model types and versions\n",
    "model_types = [\"no_reg\", \"l2\", \"jacobi\", \"svb\"]\n",
    "model_versions = [\"\", \"_no_dropout\"]\n",
    "\n",
    "model_names = []\n",
    "\n",
    "for t in model_types:\n",
    "    for v in model_versions:\n",
    "        for i in range(5):\n",
    "            model_names.append(f\"model_{t}{v}_{i}\")\n",
    "\n",
    "models = {name: ModelInfo(name) for name in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I calculate the accuracies of each model as a 95% confidence interval. I have trained five models of each variation that I use to calculate the CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold accuracy data\n",
    "accuracy_data = {}\n",
    "\n",
    "# Calculate accuracies with 95% CI for models\n",
    "for t in model_types:\n",
    "    for v in model_versions:\n",
    "        accuracies = []\n",
    "        for i in range(5):  # Assuming 5 versions of each model\n",
    "            model_name = f\"model_{t}{v}_{i}\"\n",
    "            model = models[model_name].model\n",
    "            acc = accuracy(model, test_loader, device)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        # Calculate mean accuracy\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "        # Calculate standard error\n",
    "        std_error = stats.sem(accuracies)\n",
    "\n",
    "        # Calculate confidence interval\n",
    "        CI = std_error * stats.t.ppf((1 + 0.95) / 2, len(accuracies) - 1)\n",
    "\n",
    "        # Store results in the data dictionary\n",
    "        accuracy_data[f\"{t}{v}\"] = (mean_accuracy, CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of names must match number of levels in MultiIndex.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(accuracy_data, orient\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mMean Accuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mConfidence Interval\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[39m# Reshape the DataFrame to have model types as rows and model versions as columns\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mMultiIndex\u001b[39m.\u001b[39mfrom_tuples(df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, expand\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mtolist(), names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVersion\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39munstack()\u001b[39m.\u001b[39mswaplevel(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msort_index(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:211\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    209\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m meth(self_or_cls, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:597\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    594\u001b[0m     arrs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtuples)\n\u001b[0;32m    595\u001b[0m     arrays \u001b[39m=\u001b[39m cast(List[Sequence[Hashable]], arrs)\n\u001b[1;32m--> 597\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_arrays(arrays, sortorder\u001b[39m=\u001b[39msortorder, names\u001b[39m=\u001b[39mnames)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:510\u001b[0m, in \u001b[0;36mMultiIndex.from_arrays\u001b[1;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n\u001b[0;32m    508\u001b[0m     names \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(arr, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 510\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    511\u001b[0m     levels\u001b[39m=\u001b[39mlevels,\n\u001b[0;32m    512\u001b[0m     codes\u001b[39m=\u001b[39mcodes,\n\u001b[0;32m    513\u001b[0m     sortorder\u001b[39m=\u001b[39msortorder,\n\u001b[0;32m    514\u001b[0m     names\u001b[39m=\u001b[39mnames,\n\u001b[0;32m    515\u001b[0m     verify_integrity\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    516\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:345\u001b[0m, in \u001b[0;36mMultiIndex.__new__\u001b[1;34m(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity)\u001b[0m\n\u001b[0;32m    342\u001b[0m result\u001b[39m.\u001b[39m_names \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(levels)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[39m# handles name validation\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m     result\u001b[39m.\u001b[39m_set_names(names)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m sortorder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     result\u001b[39m.\u001b[39msortorder \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(sortorder)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\bioai\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:1433\u001b[0m, in \u001b[0;36mMultiIndex._set_names\u001b[1;34m(self, names, level, validate)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLength of names must match length of level.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1432\u001b[0m     \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(names) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlevels:\n\u001b[1;32m-> 1433\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1434\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLength of names must match number of levels in MultiIndex.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1435\u001b[0m         )\n\u001b[0;32m   1437\u001b[0m \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1438\u001b[0m     level \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlevels)\n",
      "\u001b[1;31mValueError\u001b[0m: Length of names must match number of levels in MultiIndex."
     ]
    }
   ],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(\n",
    "    accuracy_data, orient=\"index\", columns=[\"Mean Accuracy\", \"Confidence Interval\"]\n",
    ")\n",
    "\n",
    "# Reshape the DataFrame to have model types as rows and model versions as columns\n",
    "df.index = pd.MultiIndex.from_tuples(\n",
    "    df.index.str.split(\"_\", expand=True).tolist(), names=[\"Type\", \"Version\"]\n",
    ")\n",
    "df = df.unstack().swaplevel(axis=1).sort_index(axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total variation is the sum of the absolute differences for neighboring pixel-values in the input images. This measures how much noise is in the images. I generate 25 different images for each model to get a good mean and standard deviation, and give the results as a tabel for each model with the eight models (models with and without dropout) as rows, and the three different zoom levels as columns. The tabel contains the mean and 95 % confidence interval for the total variation for each model at each zoom level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define zoom levels, number of images, and confidence level for confidence interval calculation\n",
    "zoom_levels = [0.025, 0.01, 0.001]\n",
    "n_images = 25\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Dataframe to store results\n",
    "df_results = pd.DataFrame(index=model_names, columns=zoom_levels)\n",
    "\n",
    "# Loop over all models\n",
    "for name in tqdm(model_names):\n",
    "    model = models[name].model # Get model\n",
    "    model.to(device)\n",
    "    tv_values = {zoom: [] for zoom in zoom_levels} # To store total variation values for each zoom level\n",
    "\n",
    "    # Genereate tv values for n_images number of images\n",
    "    for _ in range(n_images):\n",
    "        # Get random image and vectors\n",
    "        img = get_random_img(test_loader)\n",
    "        v1, v2 = generate_random_vectors(img)\n",
    "\n",
    "        # Compute the total variation of decision boundaries for the current model \n",
    "        # and the generated image at different zoom levels\n",
    "        tv_list = compute_total_variation(model, img, v1, v2, device)\n",
    "        for zoom, tv in zip(zoom_levels, tv_list):\n",
    "            tv_values[zoom].append(tv)\n",
    "    \n",
    "    # Calculate mean and 95% confidence interval for total variation values at each zoom level\n",
    "    for zoom in zoom_levels:\n",
    "        mean = np.mean(tv_values[zoom])\n",
    "        std_err = np.std(tv_values[zoom]) / np.sqrt(n_images)\n",
    "        conf_interval = stats.t.ppf((1 + confidence_level) / 2, n_images - 1) * std_err\n",
    "        df_results.loc[model_name, zoom] = f\"{mean:.2f} +/- {conf_interval:.2f}\" # Store results in dataframe\n",
    "\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
