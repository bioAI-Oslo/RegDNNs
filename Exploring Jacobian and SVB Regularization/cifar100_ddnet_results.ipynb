{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of DDNet Trained on CIFAR100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I display results in terms of accuracy on test data for DDNet models trained on the CIFAR100 dataset using various regularization techniques. The CIFAR100 data is preprocessed by normalizing using mean [0.5071, 0.4865, 0.4409] and variance [0.2009, 0.1984, 0.2023]. The batch size is 100. The model optimizes using SGD with momentum p = 0.9, and standard cross-entropy loss. Model parameters are initialized using Glorot initialization (See Glorot & Bengio 2010), expect for SVB regularization which uses orthogonal initialization. Models are trained with no regularization, L2 regularization, SVB regularization and Jacobian regularization with a dropout rate of p_drop = 0.5. I also train a model with Jacobian Regularization without Dropout. The L2 regularization coefficient and Jacobian regularization coefficient are the same as in Hoffman 2019: l2_lmbd = 0.0005 and lambda_jacobian_reg = 0.01. For SVB regularization I use the hyperparameters svb_freq=600 and svb_eps = 0.05. The learning rate starts at 0.1, and is reduced to 0.01 and 0.001 1/3 and 2/3s into training, respectively. The models are trained for 250 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy import stats\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_generators import data_loader_CIFAR100\n",
    "from model_classes import DDNet\n",
    "from tools import accuracy, ModelInfo\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST data\n",
    "train_loader, test_loader = data_loader_CIFAR100()\n",
    "\n",
    "# Summary of model\n",
    "summary_model = DDNet().to(device)\n",
    "summary(summary_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelInfo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names_set:\n\u001b[0;32m     15\u001b[0m         model_names\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m models \u001b[39m=\u001b[39m {name: ModelInfo(name, dataset) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names}\n",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names_set:\n\u001b[0;32m     15\u001b[0m         model_names\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m models \u001b[39m=\u001b[39m {name: ModelInfo(name, dataset) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m model_names}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ModelInfo' is not defined"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "dataset = \"cifar100\"\n",
    "    \n",
    "model_names_set = [\n",
    "    \"model_no_reg\",\n",
    "    \"model_l2\",\n",
    "    \"model_jacobi\",\n",
    "    \"model_jacobi_no_dropout\",\n",
    "    \"model_svb\",\n",
    "]\n",
    "model_names = []\n",
    "\n",
    "for i in range(5):\n",
    "    for name in model_names_set:\n",
    "        model_names.append(f\"{name}_{i}\")\n",
    "\n",
    "models = {name: ModelInfo(name, dataset) for name in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I calculate the accuracies of each model as a 95% confidence interval. I have trained five models of each variation that I use to calculate the CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold accuracy data\n",
    "accuracy_data = {}\n",
    "\n",
    "# Calculate accuracies with 95% CI for models\n",
    "for name in model_names_set:\n",
    "    accuracies = []\n",
    "    for i in range(5):  # Assuming 5 versions of each model\n",
    "        model_name = f\"{name}_{i}\"\n",
    "        model = models[model_name].model\n",
    "        acc = accuracy(model, test_loader, device)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "    # Calculate standard error\n",
    "    std_error = stats.sem(accuracies)\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    CI = std_error * stats.t.ppf((1 + 0.95) / 2, len(accuracies) - 1)\n",
    "\n",
    "    # Store results in the data dictionary\n",
    "    accuracy_data[f\"{name}\"] = (mean_accuracy, CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(\n",
    "    accuracy_data, orient=\"index\", columns=[\"Mean Accuracy\", \"Confidence Interval\"]\n",
    ")\n",
    "\n",
    "# Convert the numbers from decimal to percentage and round to four decimal places\n",
    "df = df.multiply(100).round(3)\n",
    "\n",
    "# Create a new column that combines Mean Accuracy and Confidence Interval\n",
    "df[\"Accuracy +/- CI (95%)\"] = df.apply(\n",
    "    lambda row: f\"{row['Mean Accuracy']} +/- {row['Confidence Interval']}\", axis=1\n",
    ")\n",
    "\n",
    "# Drop the original columns\n",
    "df = df.drop(columns=[\"Mean Accuracy\", \"Confidence Interval\"])\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total variation is a measure for roughness/complexity in images. I generate 25 different images for each model to get a good mean and standard deviation, and give the results as a tabel for each model with the eight models (models with and without dropout) as rows, and the three different zoom levels as columns. The tabel contains the mean and 95 % confidence interval for the total variation for each model at each zoom level, for both isotropic and anisotropic total variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isotropic Total Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained results\n",
    "\n",
    "# Define zoom levels\n",
    "zoom_levels = [0.025, 0.01, 0.001]\n",
    "\n",
    "# Define the models\n",
    "model_names = [\n",
    "    \"model_no_reg_0\",\n",
    "    \"model_l2_0\",\n",
    "    \"model_jacobi_0\",\n",
    "    \"model_jacobi_no_dropout_0\",\n",
    "    \"model_svb_0\",\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "cols = pd.MultiIndex.from_product([zoom_levels, [\"mean\", \"conf_interval\"]])\n",
    "df_results_isotropic = pd.DataFrame(index=model_names, columns=cols)\n",
    "\n",
    "# Loop over the selected models\n",
    "for name in tqdm(model_names):\n",
    "    # Load the results\n",
    "    with open(\n",
    "        f\"./total_variation_{dataset}_models/{name}_total_isotropic_variation.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        df_results = pickle.load(f)\n",
    "\n",
    "    # Copy the results to the dataframe\n",
    "    for zoom in zoom_levels:\n",
    "        df_results_isotropic.loc[name, (zoom, \"mean\")] = df_results.loc[\n",
    "            name, (zoom, \"mean\")\n",
    "        ]\n",
    "        df_results_isotropic.loc[name, (zoom, \"conf_interval\")] = df_results.loc[\n",
    "            name, (zoom, \"conf_interval\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Isotropic Total variation for three different zoom levels (mean and 95% CI for 50 generated images at each zoom level)\"\n",
    ")\n",
    "display(df_results_isotropic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anisotropic Total Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained results\n",
    "\n",
    "# Define zoom levels\n",
    "zoom_levels = [0.025, 0.01, 0.001]\n",
    "\n",
    "# Define the models\n",
    "model_names = [\n",
    "    \"model_no_reg_0\",\n",
    "    \"model_l2_0\",\n",
    "    \"model_jacobi_0\",\n",
    "    \"model_jacobi_no_dropout_0\",\n",
    "    \"model_svb_0\",\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "cols = pd.MultiIndex.from_product([zoom_levels, [\"mean\", \"conf_interval\"]])\n",
    "df_results_anisotropic = pd.DataFrame(index=model_names, columns=cols)\n",
    "\n",
    "# Loop over the selected models\n",
    "for name in tqdm(model_names):\n",
    "    # Load the results\n",
    "    with open(\n",
    "        f\"./total_variation_{dataset}_models/{name}_total_anisotropic_variation.pkl\",\n",
    "        \"rb\",\n",
    "    ) as f:\n",
    "        df_results = pickle.load(f)\n",
    "\n",
    "    # Copy the results to the dataframe\n",
    "    for zoom in zoom_levels:\n",
    "        df_results_anisotropic.loc[name, (zoom, \"mean\")] = df_results.loc[\n",
    "            name, (zoom, \"mean\")\n",
    "        ]\n",
    "        df_results_anisotropic.loc[name, (zoom, \"conf_interval\")] = df_results.loc[\n",
    "            name, (zoom, \"conf_interval\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Anisotropic Total variation for three different zoom levels (mean and 95% CI for 50 generated images at each zoom level)\"\n",
    ")\n",
    "display(df_results_anisotropic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
