{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "506e706e-834d-487a-8f72-13b796d57219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1562.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(3.9200)\n",
      "tensor(3.8416)\n",
      "tensor(3.7648)\n",
      "tensor(3.6895)\n",
      "tensor(3.6157)\n",
      "tensor(3.5434)\n",
      "tensor(3.4725)\n",
      "tensor(3.4031)\n",
      "tensor(3.3350)\n",
      "tensor(3.2683)\n",
      "tensor(3.2029)\n",
      "tensor(3.1389)\n",
      "tensor(3.0761)\n",
      "tensor(3.0146)\n",
      "tensor(2.9543)\n",
      "tensor(2.8952)\n",
      "tensor(2.8373)\n",
      "tensor(2.7805)\n",
      "tensor(2.7249)\n",
      "tensor(2.6704)\n",
      "tensor(2.6170)\n",
      "tensor(2.5647)\n",
      "tensor(2.5134)\n",
      "tensor(2.4631)\n",
      "tensor(2.4139)\n",
      "tensor(2.3656)\n",
      "tensor(2.3183)\n",
      "tensor(2.2719)\n",
      "tensor(2.2265)\n",
      "tensor(2.1819)\n",
      "tensor(2.1383)\n",
      "tensor(2.0955)\n",
      "tensor(2.0536)\n",
      "tensor(2.0125)\n",
      "tensor(1.9723)\n",
      "tensor(1.9329)\n",
      "tensor(1.8942)\n",
      "tensor(1.8563)\n",
      "tensor(1.8192)\n",
      "tensor(1.7828)\n",
      "tensor(1.7471)\n",
      "tensor(1.7122)\n",
      "tensor(1.6780)\n",
      "tensor(1.6444)\n",
      "tensor(1.6115)\n",
      "tensor(1.5793)\n",
      "tensor(1.5477)\n",
      "tensor(1.5167)\n",
      "tensor(1.4864)\n",
      "tensor(1.4567)\n",
      "tensor(1.4275)\n",
      "tensor(1.3990)\n",
      "tensor(1.3710)\n",
      "tensor(1.3436)\n",
      "tensor(1.3167)\n",
      "tensor(1.2904)\n",
      "tensor(1.2646)\n",
      "tensor(1.2393)\n",
      "tensor(1.2145)\n",
      "tensor(1.1902)\n",
      "tensor(1.1664)\n",
      "tensor(1.1431)\n",
      "tensor(1.1202)\n",
      "tensor(1.0978)\n",
      "tensor(1.0759)\n",
      "tensor(1.0543)\n",
      "tensor(1.0333)\n",
      "tensor(1.0126)\n",
      "tensor(0.9923)\n",
      "tensor(0.9725)\n",
      "tensor(0.9530)\n",
      "tensor(0.9340)\n",
      "tensor(0.9153)\n",
      "tensor(0.8970)\n",
      "tensor(0.8791)\n",
      "tensor(0.8615)\n",
      "tensor(0.8442)\n",
      "tensor(0.8274)\n",
      "tensor(0.8108)\n",
      "tensor(0.7946)\n",
      "tensor(0.7787)\n",
      "tensor(0.7631)\n",
      "tensor(0.7479)\n",
      "tensor(0.7329)\n",
      "tensor(0.7183)\n",
      "tensor(0.7039)\n",
      "tensor(0.6898)\n",
      "tensor(0.6760)\n",
      "tensor(0.6625)\n",
      "tensor(0.6492)\n",
      "tensor(0.6363)\n",
      "tensor(0.6235)\n",
      "tensor(0.6111)\n",
      "tensor(0.5988)\n",
      "tensor(0.5869)\n",
      "tensor(0.5751)\n",
      "tensor(0.5636)\n",
      "tensor(0.5524)\n",
      "tensor(0.5413)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "x_old = torch.tensor(2., requires_grad = True)\n",
    "N = 100\n",
    "lr = 0.01\n",
    "history = np.zeros(N)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    y = f(x_old)\n",
    "    y.backward()\n",
    "    history[i] = x_old.grad.item()\n",
    "    x_new = x_old - lr * x_old.grad\n",
    "    print(x_old.grad)\n",
    "    x_old = torch.tensor(x_new.item(), requires_grad = True)\n",
    "    \n",
    "\n",
    "#lin = np.linspace(-5, 5, 1000)\n",
    "#plt.plot(history, f(history), label = \"Model\")\n",
    "#plt.plot(lin, f(lin), label = \"Exact\")\n",
    "#plt.show()\n",
    "\n",
    "#for i in tqdm(range(N)):\n",
    "    #y = f(x)\n",
    "    #y.backward()\n",
    "    #x_copy = x.grad.item()\n",
    "    #history.append()\n",
    "    #print(x.grad)\n",
    "    #x = x - lr *x.grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffccba48-bb6e-4fa3-9db6-43057266220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "809e736f-dba0-464d-bb1a-9214a93c5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11a71098-a042-4088-9bf3-736e8e330224",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96ff50bb-37f2-4f0a-b089-cc22c16935fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddf271af-f187-4535-98a9-eefd1447de14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = x.grad\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63ec584a-a7f6-45f4-9efc-dcade35086d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c00ae649-9aee-40e9-b354-7b114f3f7517",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'zero_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'zero_grad'"
     ]
    }
   ],
   "source": [
    "x.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0f9286-f270-482d-964b-ca93c7aaa867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aslak\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fb117a9-3fec-427f-875f-6b70d31c8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "# Creating the graph\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "z = x ** 3\n",
    "z.backward() #Computes the gradient \n",
    "print(x.grad) #Prints '3' which is dz/dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "398e5b29-f127-441f-ba10-88e1002690dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fa7ad97-aece-429d-91b3-78539f312d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3., requires_grad = True)\n",
    "z = x**2\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb315fd6-9b42-4499-addf-2c3d21f9c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9becaa00-311a-4ce4-8f5c-68c8a40fb441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c08d662-8b68-43a0-a0ec-c1299f135091",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ac9069-6981-4152-a488-3130c4c8b498",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m x_vals \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m y_vals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m(gradient\u001b[38;5;241m=\u001b[39mexternal_grad)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(N)):\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m step\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mgrad\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad = True)\n",
    "external_grad = torch.tensor([1.])\n",
    "\n",
    "step = 0.01\n",
    "N = 50\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "f.backward(gradient=external_grad)\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    x = x - step*x.grad\n",
    "    f.backward(gradient=external_grad)\n",
    "    f = x*x\n",
    "    #x.detach().requires_grad_()\n",
    "    x.grad = 0\n",
    "    y_vals.append(f.item())\n",
    "    x_vals.append(x.item())\n",
    "\n",
    "plt.plot(x_vals, y_vals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
